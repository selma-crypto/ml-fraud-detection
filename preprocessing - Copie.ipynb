{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies principales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Séparation train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Métriques d'évaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, f1_score,\n",
    "    precision_score, balanced_accuracy_score,\n",
    "    matthews_corrcoef, classification_report\n",
    ")\n",
    "\n",
    "# Modèle XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier CSV\n",
    "df = pd.read_csv(\"transactions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la colonne en datetime avec gestion du fuseau horaire\n",
    "# Pandas détecte le 'Z' et met la colonne en UTC\n",
    "df[\"transaction_time\"] = pd.to_datetime(df[\"transaction_time\"], utc=True) #transformation de la colonne transaction_time en datetime pour avoir le moment exact de chaque transaction #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mapping des fuseaux horaires IANA pour les pays du dataset\n",
    "TZ_MAP = {\n",
    "    'FR': 'Europe/Paris',\n",
    "    'US': 'America/New_York',\n",
    "    'TR': 'Europe/Istanbul',\n",
    "    'PL': 'Europe/Warsaw',\n",
    "    'ES': 'Europe/Madrid',\n",
    "    'IT': 'Europe/Rome',\n",
    "    'RO': 'Europe/Bucharest',\n",
    "    'GB': 'Europe/London',\n",
    "    'NL': 'Europe/Amsterdam',\n",
    "    'DE': 'Europe/Berlin'\n",
    "}\n",
    "\n",
    "# 1. Charger le dataset (Assurez-vous que cette étape est effectuée en amont)\n",
    "# df = pd.read_csv(\"transactions.csv\")\n",
    "\n",
    "# 2. Préparation : Convertir l'heure en datetime et s'assurer qu'elle est localisée en UTC.\n",
    "df[\"transaction_time\"] = pd.to_datetime(df[\"transaction_time\"], utc=True)\n",
    "\n",
    "# 3. Fonction d'extraction et de conversion\n",
    "def get_local_feature(row, feature_type):\n",
    "    \"\"\"Convertit l'horodatage UTC en local et extrait une feature spécifique.\"\"\"\n",
    "    country_code = row['country']\n",
    "    tz_name = TZ_MAP.get(country_code, 'UTC') # Sécurité : utilise UTC si le pays n'est pas mappé\n",
    "    \n",
    "    # Convertir l'horodatage UTC au fuseau horaire local\n",
    "    local_dt = row['transaction_time'].tz_convert(tz_name)\n",
    "    \n",
    "    # Extraire la feature\n",
    "    if feature_type == 'hour':\n",
    "        return local_dt.hour\n",
    "    elif feature_type == 'dayofweek':\n",
    "        return local_dt.dayofweek\n",
    "    elif feature_type == 'day_name':\n",
    "        return local_dt.day_name()\n",
    "    elif feature_type == 'month':\n",
    "        return local_dt.month\n",
    "    elif feature_type == 'year':\n",
    "        return local_dt.year\n",
    "    elif feature_type == 'day':\n",
    "        return local_dt.day\n",
    "    # Retourner l'objet datetime local complet (utile pour la vérification)\n",
    "    elif feature_type == 'full_local':\n",
    "        return local_dt\n",
    "\n",
    "# 4. Extraction des features locales dans le DataFrame :\n",
    "df[\"hour_local\"] = df.apply(lambda row: get_local_feature(row, 'hour'), axis=1)\n",
    "df[\"dayofweek_local\"] = df.apply(lambda row: get_local_feature(row, 'dayofweek'), axis=1)\n",
    "# df[\"month_local\"] = df.apply(lambda row: get_local_feature(row, 'month'), axis=1)\n",
    "# df[\"year_local\"] = df.apply(lambda row: get_local_feature(row, 'year'), axis=1)\n",
    "df[\"day_local\"] = df.apply(lambda row: get_local_feature(row, 'day'), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"user_id\",\"transaction_time\"]).reset_index(drop=True) #Classe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df[\"hour\"] = df[\"transaction_time\"].dt.hour #on extrait l'heure de la transaction pour repérer les périodes ou la fraude est plus fréquente #OK\n",
    "\n",
    "# df[\"hour\"] = df[\"local_tx_date\"].dt.hour #on extrait l'heure de la transaction pour repérer les périodes ou la fraude est plus fréquente #OK\n",
    "\n",
    "# df[\"dayofweek\"] = df[\"transaction_time\"].dt.dayofweek #on extrait le jour de la semaine pour voir si certaine journée (comme le week end) on plus de fraude #OK\n",
    "\n",
    "# df[\"dayofweek\"] = df[\"local_tx_date\"].dt.dayofweek #on extrait le jour de la semaine pour voir si certaine journée (comme le week end) on plus de fraude #OK\n",
    "\n",
    "df[\"is_night\"] = ((df[\"hour_local\"] >= 22) | (df[\"hour_local\"] <= 5)) * 1 #ici créatiion d'une variable qui vaudra 1 si la transaction a lieu la nuit (de 22h a 5h du matin) le test logique  donnera \"True\" si entre 0 et 5 sinon False et ensuite transformation du resultat en 1/0  utilisable par un modele #OK\n",
    "\n",
    "df[\"avg_amount_user_past\"] = (df.groupby(\"user_id\")[\"amount\"].expanding().mean().shift(1).reset_index(level=0, drop=True)) # calcule la somme cumulée et le nombre de transactions passées, on les divise, et on obtient la moyenne passée sans aucune donnée future.\n",
    "\n",
    "df[\"amount_diff_user_avg\"] = df[\"amount\"] - df[\"avg_amount_user_past\"]# mesure si le montant actuel est différent du montant moyen habituel de l'utilisateur # OK\n",
    "\n",
    "df[\"is_new_account\"] = (df[\"account_age_days\"] < 30) * 1 #verifie si c'est un nouveau compte ou pas , test logique pour voir si il a moin de 30 jour et transformation du resultat en 1/0 #OK\n",
    "\n",
    "df[\"security_mismatch_score\"] = (df[\"avs_match\"] == 0) * 1 + (df[\"cvv_result\"] == 0) * 1 # ici ca calcule un score de risque en comptant combien de vérification on échoué (avs ou cvv) un total de 0,1,2 #OK\n",
    "#________________________________________________________________________________________________________________________________________\n",
    "df[\"user_fraud_count\"] = (df.groupby(\"user_id\")[\"is_fraud\"].cumsum().shift(1).fillna(0)) #calcul combien de fraudes un utilisateur a deja fait au total #ok\n",
    "\n",
    "df[\"user_has_fraud_history\"] = (df[\"user_fraud_count\"] > 0) * 1 # montre si l'utilisateur a deja fraudé au moin 1 fios 0/1\n",
    "\n",
    "df[\"user_tx_count\"] = df.groupby(\"user_id\").cumcount() #combien de transaction l'utilisateur avait avant celle ci (compteur historique)\n",
    "\n",
    "df[\"user_fraud_rate\"] = (df[\"user_fraud_count\"] / df[\"user_tx_count\"]).fillna(0) #nombre de fraude déjà commises avant celle ci\n",
    "df[\"user_fraud_rate\"] = df[\"user_fraud_rate\"].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "#_________________________________________________________________________________________________________________________________________\n",
    "\n",
    "df[\"country_bin_mismatch\"] = (df[\"country\"] != df[\"bin_country\"]) * 1 #verifie que le pays de la carte (le bin) ne correspond pas au pays de la transaction 0/1\n",
    "\n",
    "df[\"distance_amount_ratio\"] = df[\"shipping_distance_km\"] / (df[\"amount\"] + 1) #mesure si la distance d'expedition est plus importante que le montant de la transaction\n",
    "\n",
    "df[\"amount_delta_prev\"] = df.groupby(\"user_id\")[\"amount\"].diff().fillna(0) #mesure la différence entre le montant actuel et le montant de la transaction précédente du meme utilisateur pour reperer les changement de comportement\n",
    "#__________________________________________________________________________________________________________________________________________\n",
    "df[\"channel_changed\"] = (df[\"channel\"] != df.groupby(\"user_id\")[\"channel\"].shift()).astype(int)# vaux 1 si le canal de transaction change #BINAIRE NECESSAIRE\n",
    "\n",
    "#__________________________________________________________________________________________________________________________________________\n",
    "df[\"time_since_last\"] = df.groupby(\"user_id\")[\"transaction_time\"].diff().dt.total_seconds() #mesure le nombre de secondes entre la transaction actuelle et la derniere\n",
    "\n",
    "df[\"transaction_count_cum\"] = df.groupby(\"user_id\").cumcount() + 1\n",
    "#________________________________________________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tri (Toujours nécessaire)\n",
    "df = df.sort_values([\"user_id\", \"transaction_time\"])\n",
    "\n",
    "#  Fonction corrigée sans shift, avec closed='left'\n",
    "def get_rolling_count_safe(g, window):\n",
    "    # closed='left' signifie : regarde du passé jusqu'à maintenant,\n",
    "    # mais EXCLUT la transaction actuelle du compte.\n",
    "    return pd.Series(\n",
    "        g.set_index(\"transaction_time\")[\"amount\"]\n",
    "         .rolling(window, closed='left')\n",
    "         .count()\n",
    "         .values,\n",
    "        index=g.index\n",
    "    )\n",
    "\n",
    "#  Application\n",
    "# On sélectionne les colonnes avant le apply pour éviter les warnings/erreurs\n",
    "cols_needed = [\"transaction_time\", \"amount\"]\n",
    "\n",
    "df[\"tx_last_24h\"] = df.groupby(\"user_id\", group_keys=False)[cols_needed].apply(\n",
    "    lambda g: get_rolling_count_safe(g, \"24h\")\n",
    ")\n",
    "\n",
    "df[\"tx_last_7d\"] = df.groupby(\"user_id\", group_keys=False)[cols_needed].apply(\n",
    "    lambda g: get_rolling_count_safe(g, \"7d\")\n",
    ")\n",
    "\n",
    "df[\"tx_last_30d\"] = df.groupby(\"user_id\", group_keys=False)[cols_needed].apply(\n",
    "    lambda g: get_rolling_count_safe(g, \"30d\")\n",
    ")\n",
    "\n",
    "# Remplacer les NaN (premières lignes) par 0\n",
    "df[[\"tx_last_24h\", \"tx_last_7d\", \"tx_last_30d\"]] = df[[\"tx_last_24h\", \"tx_last_7d\", \"tx_last_30d\"]].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RÉPARATION : On remet l'index à zéro pour récupérer 'transaction_time' si elle était cachée\n",
    "df = df.reset_index()\n",
    "\n",
    "# Si 'index' a été créé en trop lors du reset, on le supprime (optionnel mais propre)\n",
    "if \"index\" in df.columns:\n",
    "    df = df.drop(columns=[\"index\"])\n",
    "\n",
    "#  S'assurer que c'est bien une date (sinon le rolling plante)\n",
    "df[\"transaction_time\"] = pd.to_datetime(df[\"transaction_time\"])\n",
    "\n",
    "#  Tri (Obligatoire)\n",
    "df = df.sort_values([\"user_id\", \"transaction_time\"])\n",
    "\n",
    "#  Calcul Optimisé (Boucle propre)\n",
    "# On utilise closed='left' pour exclure la transaction actuelle du compte (remplace le shift)\n",
    "windows = {\"24h\": \"tx_last_24h\", \"7d\": \"tx_last_7d\", \"30d\": \"tx_last_30d\"}\n",
    "cols_needed = [\"transaction_time\", \"amount\"]\n",
    "\n",
    "for window, col_name in windows.items():\n",
    "    df[col_name] = df.groupby(\"user_id\", group_keys=False)[cols_needed].apply(\n",
    "        lambda g: pd.Series(\n",
    "            g.set_index(\"transaction_time\")[\"amount\"]\n",
    "             .rolling(window, closed='left')\n",
    "             .count()\n",
    "             .values,\n",
    "            index=g.index\n",
    "        )\n",
    "    ).fillna(0)\n",
    "\n",
    "# Vérification finale\n",
    "print(df[[\"user_id\", \"transaction_time\", \"tx_last_24h\", \"tx_last_7d\", \"tx_last_30d\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"transaction_time\"]) #Suppression de la colonne transaction time\n",
    "df.drop(columns=[\"transaction_id\"], inplace=True)\n",
    "df = pd.get_dummies(\n",
    "    df, columns=[\"country\", \"bin_country\", \"channel\", \"merchant_category\",],\n",
    "    drop_first=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, balanced_accuracy_score, matthews_corrcoef, confusion_matrix, classification_report\n",
    "# Target\n",
    "y = df[\"is_fraud\"]\n",
    "\n",
    "# Features (tout sauf la target)\n",
    "X = df.drop(columns=[\"is_fraud\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul poids de classe\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    min_child_weight=12,   # limite les feuilles trop petites → moins d’overfit\n",
    "    gamma=3,               # empêche les splits inutiles → régularisation\n",
    "    subsample=0.7,         # chaque arbre voit moins d’échantillons → généralisation\n",
    "    colsample_bytree=0.7,  # chaque arbre voit moins de features → moins d’overfit\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=12,   # moins agressif que 15 → meilleur équilibre\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time_since_last\"] = df[\"time_since_last\"].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Predict\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred  = model.predict(X_test)\n",
    "\n",
    "# 3. Metrics\n",
    "print(\"===== TRAIN RESULTS =====\")\n",
    "print(\"Accuracy :\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Recall   :\", recall_score(y_train, y_train_pred))\n",
    "print(\"Precision:\", precision_score(y_train, y_train_pred))\n",
    "print(\"F1-score :\", f1_score(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\n===== TEST RESULTS =====\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Recall   :\", recall_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"F1-score :\", f1_score(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Récupération des importances XGBoost\n",
    "importance = model.get_booster().get_score(importance_type='gain')\n",
    "\n",
    "#  DataFrame trié\n",
    "importance_df = pd.DataFrame(\n",
    "    importance.items(),\n",
    "    columns=[\"feature\", \"importance\"]\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "#  Affichage du tableau complet\n",
    "display(importance_df)\n",
    "\n",
    "#  Plot propre sans warning\n",
    "plt.figure(figsize=(10, len(importance_df) * 0.3))  # hauteur auto-adaptée\n",
    "sns.barplot(\n",
    "    data=importance_df,\n",
    "    x=\"importance\",\n",
    "    y=\"feature\",\n",
    "    hue=\"feature\",        # pour éviter le warning\n",
    "    dodge=False,\n",
    "    legend=False,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Feature Importance (gain) – All Features\")\n",
    "plt.xlabel(\"Importance (gain)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Conversion des booléens en int (sinon heatmap plante)\n",
    "df_corr = df.copy()\n",
    "bool_cols = df_corr.select_dtypes(include=['bool']).columns\n",
    "df_corr[bool_cols] = df_corr[bool_cols].astype(int)\n",
    "\n",
    "# 2. Calcul des corrélations\n",
    "corr_matrix = df_corr.corr()\n",
    "\n",
    "# 3. Heatmap\n",
    "plt.figure(figsize=(22, 18))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    cmap=\"coolwarm\",\n",
    "    annot=False,\n",
    "    vmin=-1, vmax=1,\n",
    "    linewidths=0.1\n",
    ")\n",
    "\n",
    "plt.title(\"Heatmap de Corrélation – Dataset Fraude\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Obtenir les corrélations avec la variable cible\n",
    "target_corr = corr_matrix['is_fraud'].drop('is_fraud')\n",
    "\n",
    "# 2. Trier par valeur absolue et ne garder que les 20 premières\n",
    "top_features = target_corr.abs().sort_values(ascending=False).head(20).index\n",
    "target_corr_top = target_corr.loc[top_features]\n",
    "\n",
    "# 3. Visualisation (Bar Plot au lieu de Heatmap)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=target_corr_top.values, y=target_corr_top.index, palette=\"coolwarm\")\n",
    "plt.title(\"Top 20 Corrélations avec 'is_fraud'\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Typologies de fraude : antécédents, géographiques, montants, autres facteurs (canal web, promo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Utiliser clustermap au lieu de heatmap\n",
    "plt.figure(figsize=(22, 18))\n",
    "sns.clustermap(\n",
    "    corr_matrix,\n",
    "    cmap=\"coolwarm\",\n",
    "    annot=False,\n",
    "    vmin=-1, vmax=1,\n",
    "    linewidths=0.1,\n",
    "    figsize=(18, 18) # Utiliser figsize dans clustermap lui-même\n",
    ")\n",
    "plt.suptitle(\"Clustermap de Corrélation – Regroupement des Caractéristiques\", y=1.02, fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fonction objectif Optuna\n",
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 5, 50),\n",
    "\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    # Score orienté fraude : Recall + F1\n",
    "    recall = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "\n",
    "    # On maximise : trade-off F1 + Recall\n",
    "    return (0.6 * recall) + (0.4 * f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score:\", study.best_value)\n",
    "print(\"Best Params:\", study.best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 1. Modèle XGBoost – Best Optuna Params\n",
    "# ===============================\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=360,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.08988925976027803,\n",
    "    gamma=6.832453848633769,\n",
    "    min_child_weight=16,\n",
    "    subsample=0.9989920858950934,\n",
    "    colsample_bytree=0.5418940975093512,\n",
    "    scale_pos_weight=5.003569800495007,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 2. Entraînement\n",
    "# ===============================\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ===============================\n",
    "# 3. Prédictions (seuil par défaut 0.5)\n",
    "# ===============================\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred  = model.predict(X_test)\n",
    "\n",
    "# ===============================\n",
    "# 4. Scores\n",
    "# ===============================\n",
    "print(\"===== TRAIN RESULTS =====\")\n",
    "print(\"Recall   :\", recall_score(y_train, y_train_pred))\n",
    "print(\"Precision:\", precision_score(y_train, y_train_pred))\n",
    "print(\"F1-score :\", f1_score(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\n===== TEST RESULTS =====\")\n",
    "print(\"Recall   :\", recall_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"F1-score :\", f1_score(y_test, y_test_pred))\n",
    "\n",
    "# ===============================\n",
    "# 5. Confusion Matrix\n",
    "# ===============================\n",
    "print(\"\\n===== CONFUSION MATRIX =====\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\n===== CLASSIFICATION REPORT =====\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=360,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.08988925976027803,\n",
    "        gamma=6.832453848633769,\n",
    "        min_child_weight=16,\n",
    "        subsample=0.9989920858950934,\n",
    "        colsample_bytree=0.5418940975093512,\n",
    "        scale_pos_weight=5.003569800495007,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=12,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        l2_regularization=1.0,\n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        iterations=400,\n",
    "        depth=6,\n",
    "        learning_rate=0.05,\n",
    "        loss_function=\"Logloss\",\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "def evaluate_model(name, model):\n",
    "    preds = model.predict(X_test)\n",
    "    recall = recall_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    results.append([name, recall, precision, f1])\n",
    "\n",
    "\n",
    "print(\"=== TRAINING MODELS ===\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_model(name, model)\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Recall\", \"Precision\", \"F1\"])\n",
    "\n",
    "print(\"\\n=== RESULTS COMPARISON ===\")\n",
    "print(results_df.sort_values(\"F1\", ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SMOTETomek robuste\n",
    "# Gère automatiquement les datasets :\n",
    "# - full numériques\n",
    "# - mixtes numériques / catégorielles\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTENC, SMOTE\n",
    "\n",
    "# ============================================================\n",
    "# 1. Copies de sécurité\n",
    "# ============================================================\n",
    "\n",
    "X_train_res = X_train.copy()\n",
    "y_train_res = y_train.copy()\n",
    "\n",
    "# ============================================================\n",
    "# 2. Détection des types de colonnes\n",
    "# ============================================================\n",
    "\n",
    "cat_cols = X_train_res.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X_train_res.columns.difference(cat_cols).tolist()\n",
    "\n",
    "print(\"Colonnes catégorielles :\", cat_cols)\n",
    "print(\"Colonnes numériques :\", num_cols)\n",
    "\n",
    "# ============================================================\n",
    "# 3. Imputation des valeurs manquantes\n",
    "# ============================================================\n",
    "\n",
    "# Numériques -> médiane\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_res[num_cols] = num_imputer.fit_transform(X_train_res[num_cols])\n",
    "\n",
    "# Catégorielles -> valeur la plus fréquente (si présentes)\n",
    "if len(cat_cols) > 0:\n",
    "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    X_train_res[cat_cols] = cat_imputer.fit_transform(X_train_res[cat_cols])\n",
    "\n",
    "print(\"Nombre total de NaN restants :\", X_train_res.isna().sum().sum())\n",
    "\n",
    "# ============================================================\n",
    "# 4. Sélection automatique de la stratégie SMOTE\n",
    "# ============================================================\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    # Cas colonnes catégorielles non encodées\n",
    "    cat_indices = [X_train_res.columns.get_loc(col) for col in cat_cols]\n",
    "    print(\"Stratégie : SMOTENC + TomekLinks\")\n",
    "\n",
    "    smote = SMOTENC(\n",
    "        categorical_features=cat_indices,\n",
    "        random_state=42,\n",
    "        k_neighbors=5\n",
    "    )\n",
    "else:\n",
    "    # Cas full numérique\n",
    "    print(\"Stratégie : SMOTE classique + TomekLinks\")\n",
    "\n",
    "    smote = SMOTE(\n",
    "        random_state=42,\n",
    "        k_neighbors=5\n",
    "    )\n",
    "\n",
    "smote_tomek = SMOTETomek(\n",
    "    smote=smote,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5. Resampling (TRAIN uniquement)\n",
    "# ============================================================\n",
    "\n",
    "X_train_balanced, y_train_balanced = smote_tomek.fit_resample(\n",
    "    X_train_res,\n",
    "    y_train_res\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6. Résumé final\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n================ RÉSULTAT =================\")\n",
    "print(\n",
    "    f\"Avant  : {X_train_res.shape[0]:,} lignes | \"\n",
    "    f\"{y_train_res.sum():,} fraudes ({y_train_res.mean():.2%})\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Après  : {X_train_balanced.shape[0]:,} lignes | \"\n",
    "    f\"{y_train_balanced.sum():,} fraudes ({y_train_balanced.mean():.2%})\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# XGBOOST FINAL — FRAUDE DETECTION (SETUP VALIDÉ)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 1. SCALE_POS_WEIGHT (SUR TRAIN BALANCÉ)\n",
    "# ============================================================\n",
    "\n",
    "neg, pos = np.bincount(y_train_balanced)\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "print(\"scale_pos_weight :\", scale_pos_weight)\n",
    "\n",
    "# ============================================================\n",
    "# 2. MODÈLE XGBOOST — PARAMÈTRES OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",                 # adapté dataset déséquilibré\n",
    "    n_estimators=360,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.08988925976027803,\n",
    "    gamma=6.832453848633769,\n",
    "    min_child_weight=16,\n",
    "    subsample=0.9989920858950934,\n",
    "    colsample_bytree=0.5418940975093512,\n",
    "    scale_pos_weight=scale_pos_weight,   # cohérent avec train rééquilibré\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3. ENTRAÎNEMENT (TRAIN SMOTÉ UNIQUEMENT)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nEntraînement XGBoost...\")\n",
    "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# ============================================================\n",
    "# 4. PRÉDICTIONS SUR LE TEST RÉEL (NON SMOTÉ)\n",
    "# ============================================================\n",
    "\n",
    "y_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ============================================================\n",
    "# 5. AJUSTEMENT DU SEUIL (ORIENTÉ RECALL)\n",
    "# ============================================================\n",
    "\n",
    "threshold = 0.30\n",
    "y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "print(\"Seuil utilisé :\", threshold)\n",
    "\n",
    "# ============================================================\n",
    "# 6. ÉVALUATION FINALE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n================ PERFORMANCE TEST =================\")\n",
    "\n",
    "print(\"ROC AUC :\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "print(\"\\nConfusion Matrix :\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Matrice de confusion brute\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Normalisation par ligne (vraie classe)\n",
    "cm_percent = cm.astype(float) / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "# Affichage\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(cm_percent)\n",
    "plt.colorbar(format=\"%.0f%%\")\n",
    "\n",
    "plt.xticks([0, 1], [\"Non fraude\", \"Fraude\"])\n",
    "plt.yticks([0, 1], [\"Non fraude\", \"Fraude\"])\n",
    "\n",
    "plt.xlabel(\"Prédiction\")\n",
    "plt.ylabel(\"Vraie classe\")\n",
    "plt.title(\"Matrice de confusion (%) – XGBoost (seuil ajusté)\")\n",
    "\n",
    "# Valeurs en %\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{cm_percent[i, j]:.2f}%\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=11\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La matrice de confusion normalisée montre que le modèle laisse passer plus de 99,6 % des transactions légitimes tout en détectant plus de 81 % des fraudes, ce qui constitue un compromis pertinent entre risque financier et impact client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des importances (gain)\n",
    "importances = xgb_model.get_booster().get_score(importance_type=\"gain\")\n",
    "\n",
    "# DataFrame\n",
    "fi_df = pd.DataFrame({\n",
    "    \"Feature\": importances.keys(),\n",
    "    \"Gain\": importances.values()\n",
    "})\n",
    "\n",
    "# Normalisation en %\n",
    "fi_df[\"Importance_%\"] = fi_df[\"Gain\"] / fi_df[\"Gain\"].sum() * 100\n",
    "\n",
    "# Sélection du Top 15\n",
    "fi_top15 = fi_df.sort_values(\"Importance_%\", ascending=False).head(15)\n",
    "\n",
    "# Tri pour affichage horizontal\n",
    "fi_top15 = fi_top15.sort_values(\"Importance_%\", ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(fi_top15[\"Feature\"], fi_top15[\"Importance_%\"])\n",
    "plt.xlabel(\"Importance (%)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance – XGBoost (Top 15, gain en %)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " L’analyse d’importance montre que le modèle se base principalement sur des signaux de sécurité du paiement et de cohérence transactionnelle, complétés par l’historique utilisateur et le contexte, ce qui est conforme aux pratiques antifraude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# ROC + AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Trouver le point correspondant au seuil choisi (ex: 0.30)\n",
    "threshold_target = 0.30\n",
    "idx = np.argmin(np.abs(thresholds - threshold_target))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"XGBoost ROC (AUC = {auc_score:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random\")\n",
    "\n",
    "# Annotation du seuil\n",
    "plt.scatter(fpr[idx], tpr[idx])\n",
    "plt.text(\n",
    "    fpr[idx], tpr[idx],\n",
    "    f\"  seuil={threshold_target}\",\n",
    "    verticalalignment=\"bottom\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"ROC Curve – XGBoost (test non smoté)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La courbe ROC montre que le modèle discrimine très bien les classes avec une AUC proche de 0,98. Le seuil opérationnel n’est pas choisi sur la ROC mais en fonction du compromis métier, ici orienté recall, ce qui est matérialisé par le point à 0,3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_random_transaction(model, X_data, y_data, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Sélectionne une transaction au hasard, prédit la fraude\n",
    "    et explique la décision via SHAP, avec comparaison vérité terrain.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 1. Sélection aléatoire d'une observation\n",
    "    # ============================================================\n",
    "\n",
    "    random_idx = np.random.choice(X_data.index)\n",
    "    row_data = X_data.loc[[random_idx]]  # format DataFrame\n",
    "    true_label = y_data.loc[random_idx]\n",
    "\n",
    "    # Conversion en JSON lisible\n",
    "    row_dict = row_data.iloc[0].astype(object).to_dict()\n",
    "    for k, v in row_dict.items():\n",
    "        if isinstance(v, (np.integer, np.floating)):\n",
    "            row_dict[k] = float(v)\n",
    "\n",
    "    json_output = json.dumps(row_dict, indent=4)\n",
    "\n",
    "    print(f\"--- OBSERVATION (INDEX: {random_idx}) ---\")\n",
    "    print(f\"Vraie classe : {'FRAUDE' if true_label == 1 else 'NON FRAUDE'}\")\n",
    "    print(\"Données (JSON) :\")\n",
    "    print(json_output)\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # ============================================================\n",
    "    # 2. Prédiction du modèle\n",
    "    # ============================================================\n",
    "\n",
    "    prob_fraud = model.predict_proba(row_data)[:, 1][0]\n",
    "    pred_label = 1 if prob_fraud >= threshold else 0\n",
    "\n",
    "    print(f\"\\n--- PRÉDICTION (Seuil = {threshold}) ---\")\n",
    "    print(f\"Probabilité de fraude : {prob_fraud:.4f}\")\n",
    "    print(f\"Classe prédite        : {'FRAUDE' if pred_label == 1 else 'NON FRAUDE'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # ============================================================\n",
    "    # 3. COMPARAISON PRÉDICTION vs RÉALITÉ (IMPORTANT)\n",
    "    # ============================================================\n",
    "\n",
    "    print(\"\\n--- VERDICT ---\")\n",
    "\n",
    "    if pred_label == true_label:\n",
    "        if pred_label == 1:\n",
    "            verdict = \"VRAI POSITIF (fraude correctement détectée)\"\n",
    "        else:\n",
    "            verdict = \"VRAI NÉGATIF (transaction légitime correctement classée)\"\n",
    "    else:\n",
    "        if pred_label == 1:\n",
    "            verdict = \"FAUX POSITIF (transaction légitime bloquée à tort)\"\n",
    "        else:\n",
    "            verdict = \"FAUX NÉGATIF (fraude non détectée)\"\n",
    "\n",
    "    print(f\"Résultat : {verdict}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # ============================================================\n",
    "    # 4. EXPLICATION SHAP (locale)\n",
    "    # ============================================================\n",
    "\n",
    "    print(\"\\n--- EXPLICATION DE LA DÉCISION (SHAP) ---\")\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer(row_data)\n",
    "\n",
    "    contributions = pd.DataFrame({\n",
    "        \"Feature\": X_data.columns,\n",
    "        \"Valeur_Feature\": row_data.iloc[0].values,\n",
    "        \"Impact_SHAP\": shap_values.values[0]\n",
    "    })\n",
    "\n",
    "    contributions[\"Abs_Impact\"] = contributions[\"Impact_SHAP\"].abs()\n",
    "\n",
    "    top_contributions = (\n",
    "        contributions\n",
    "        .sort_values(by=\"Abs_Impact\", ascending=False)\n",
    "        .head(10)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(\"Top 10 des facteurs ayant influencé la décision :\")\n",
    "    display(top_contributions[[\"Feature\", \"Valeur_Feature\", \"Impact_SHAP\"]])\n",
    "\n",
    "    # ============================================================\n",
    "    # 5. Visualisation SHAP (Waterfall)\n",
    "    # ============================================================\n",
    "\n",
    "    plt.figure()\n",
    "    shap.plots.waterfall(shap_values[0], max_display=10, show=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXÉCUTION\n",
    "# ============================================================\n",
    "\n",
    "# Assure-toi que xgb_model, X_test et y_test existent\n",
    "analyze_random_transaction(\n",
    "    model=xgb_model,\n",
    "    X_data=X_test,\n",
    "    y_data=y_test,\n",
    "    threshold=0.3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
